{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-17T20:44:32.180078Z","iopub.execute_input":"2023-09-17T20:44:32.180376Z","iopub.status.idle":"2023-09-17T20:44:32.604945Z","shell.execute_reply.started":"2023-09-17T20:44:32.180349Z","shell.execute_reply":"2023-09-17T20:44:32.604000Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torch-summary","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:44:32.606831Z","iopub.execute_input":"2023-09-17T20:44:32.607551Z","iopub.status.idle":"2023-09-17T20:44:45.415708Z","shell.execute_reply.started":"2023-09-17T20:44:32.607515Z","shell.execute_reply":"2023-09-17T20:44:45.414634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as T","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:44:45.417974Z","iopub.execute_input":"2023-09-17T20:44:45.418381Z","iopub.status.idle":"2023-09-17T20:44:49.062247Z","shell.execute_reply.started":"2023-09-17T20:44:45.418345Z","shell.execute_reply":"2023-09-17T20:44:49.061074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DATA PREPARATION","metadata":{}},{"cell_type":"code","source":"transforms = T.Compose([\n    #T.ToPILImage(),\n    T.Resize((224,224)),\n    T.RandomHorizontalFlip(p=0.5),   # Randomly flip horizontally with 50% probability\n    T.RandomVerticalFlip(p=0.5),\n    T.RandomRotation(degrees=10),     # Randomly rotate by up to 10 degrees\n    T.ToTensor(),                    # Convert the image to a tensor\n    #T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n])","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:44:49.064584Z","iopub.execute_input":"2023-09-17T20:44:49.065001Z","iopub.status.idle":"2023-09-17T20:44:49.073515Z","shell.execute_reply.started":"2023-09-17T20:44:49.064974Z","shell.execute_reply":"2023-09-17T20:44:49.071247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = torchvision.datasets.ImageFolder(root='/kaggle/input/brain-mri-images-for-brain-tumor-detection/brain_tumor_dataset/', transform=transforms)\ntrain_data","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:44:49.074765Z","iopub.execute_input":"2023-09-17T20:44:49.075257Z","iopub.status.idle":"2023-09-17T20:44:49.093011Z","shell.execute_reply.started":"2023-09-17T20:44:49.075222Z","shell.execute_reply":"2023-09-17T20:44:49.091711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_labels = {'0': 'no', '1': 'yes'}","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:44:49.094761Z","iopub.execute_input":"2023-09-17T20:44:49.095413Z","iopub.status.idle":"2023-09-17T20:44:49.100031Z","shell.execute_reply.started":"2023-09-17T20:44:49.095381Z","shell.execute_reply":"2023-09-17T20:44:49.098797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SAMPLE IMAGE\n\nLet's take a look at a sample image and its label","metadata":{}},{"cell_type":"code","source":"sample_image_tensor, sample_label = train_data.__getitem__(97)\n#print(sample_img_tensor.shape)\n#plt.imshow(sample_img_tensor.permute(1, 2, 0))\nplt.imshow(sample_image_tensor.permute(1, 2, 0))\nplt.title(encoded_labels[str(sample_label)])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:44:49.101608Z","iopub.execute_input":"2023-09-17T20:44:49.102002Z","iopub.status.idle":"2023-09-17T20:44:49.521217Z","shell.execute_reply.started":"2023-09-17T20:44:49.101925Z","shell.execute_reply":"2023-09-17T20:44:49.520311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TRAIN_VAL SPLIT","metadata":{}},{"cell_type":"code","source":"split = int(np.floor(0.75*train_data.__len__()))\ntrain_set, val_set = torch.utils.data.random_split(train_data, [split, train_data.__len__()-split])","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:44:49.522237Z","iopub.execute_input":"2023-09-17T20:44:49.522594Z","iopub.status.idle":"2023-09-17T20:44:49.533178Z","shell.execute_reply.started":"2023-09-17T20:44:49.522561Z","shell.execute_reply":"2023-09-17T20:44:49.532180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_set), len(val_set)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:44:49.536672Z","iopub.execute_input":"2023-09-17T20:44:49.537318Z","iopub.status.idle":"2023-09-17T20:44:49.549472Z","shell.execute_reply.started":"2023-09-17T20:44:49.537289Z","shell.execute_reply":"2023-09-17T20:44:49.548399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nbatch_size = 16\ntrain_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\nval_dataloader = DataLoader(val_set, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:44:49.557219Z","iopub.execute_input":"2023-09-17T20:44:49.557883Z","iopub.status.idle":"2023-09-17T20:44:49.565693Z","shell.execute_reply.started":"2023-09-17T20:44:49.557843Z","shell.execute_reply":"2023-09-17T20:44:49.564616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.utils import make_grid\n\ntrain_features, train_labels = next(iter(train_dataloader))\n\ngrid = make_grid(train_features)\nplt.imshow(grid.permute(1, 2, 0))","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:44:49.567375Z","iopub.execute_input":"2023-09-17T20:44:49.569518Z","iopub.status.idle":"2023-09-17T20:44:50.170859Z","shell.execute_reply.started":"2023-09-17T20:44:49.569489Z","shell.execute_reply":"2023-09-17T20:44:50.169957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_features.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:44:50.172173Z","iopub.execute_input":"2023-09-17T20:44:50.173232Z","iopub.status.idle":"2023-09-17T20:44:50.179973Z","shell.execute_reply.started":"2023-09-17T20:44:50.173196Z","shell.execute_reply":"2023-09-17T20:44:50.179031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_features, val_labels = next(iter(val_dataloader))\n\ngrid = make_grid(val_features)\nplt.imshow(grid.permute(1, 2, 0))","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:44:50.181510Z","iopub.execute_input":"2023-09-17T20:44:50.182171Z","iopub.status.idle":"2023-09-17T20:44:50.821953Z","shell.execute_reply.started":"2023-09-17T20:44:50.182138Z","shell.execute_reply":"2023-09-17T20:44:50.821003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DENSENET ARCHITECTURE","metadata":{}},{"cell_type":"markdown","source":"### TRANSITION LAYERS:\n\nDense blocks are connected with each other using transition layers, which do convolution and pooling to downsample the image. It simply consists of a Bnorm and a 1x1 Conv layer followed by a 2x2 average pooling layer.","metadata":{}},{"cell_type":"code","source":"class Transition_Block(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(Transition_Block, self).__init__()\n        \n        self.bnorm = nn.BatchNorm2d(in_channels)\n        self.relu = nn.ReLU()\n        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1)\n        self.avgpool = nn.AvgPool2d(kernel_size=2, stride=2)\n        \n    def forward(self, x):\n        \n        out = self.relu(self.bnorm(x))\n        out = self.conv1(out)\n        out = self.avgpool(out)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:44:50.823348Z","iopub.execute_input":"2023-09-17T20:44:50.823867Z","iopub.status.idle":"2023-09-17T20:44:50.831972Z","shell.execute_reply.started":"2023-09-17T20:44:50.823835Z","shell.execute_reply":"2023-09-17T20:44:50.831054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BottleNeck_Layer(nn.Module):\n    def __init__(self, in_channels, growth_rate):\n        super(BottleNeck_Layer, self).__init__()\n        \n        inter_channels = 4 * growth_rate\n        \n        self.conv1 = nn.Sequential(\n                        nn.BatchNorm2d(in_channels),\n                        nn.ReLU(),\n                        nn.Conv2d(in_channels, inter_channels, kernel_size=1))\n        \n        self.conv2 = nn.Sequential(\n                        nn.BatchNorm2d(inter_channels),\n                        nn.ReLU(),\n                        nn.Conv2d(inter_channels, growth_rate, kernel_size=3, padding=1))\n        \n    def forward(self, x):\n            \n        out = self.conv1(x)\n        out = self.conv2(out)\n        #print(x.shape, out.shape)\n        out = torch.cat((x, out), 1)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:44:50.833438Z","iopub.execute_input":"2023-09-17T20:44:50.833812Z","iopub.status.idle":"2023-09-17T20:44:50.845066Z","shell.execute_reply.started":"2023-09-17T20:44:50.833768Z","shell.execute_reply":"2023-09-17T20:44:50.844140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DenseNet_Model_Torch(nn.Module):\n    def __init__(self, block, num_layers, growth_rate, num_classes):\n        super(DenseNet_Model_Torch, self).__init__()\n        \n        self.conv1 = nn.Sequential(\n                        nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3),\n                        nn.BatchNorm2d(64),\n                        nn.ReLU())\n    \n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        in_channels=64\n        \n        self.dense1 = self.make_layer(BottleNeck_Layer, in_channels=in_channels, growth_rate=growth_rate, num_layers=num_layers[0])\n        in_channels += num_layers[0]*growth_rate                                               ### in_channels => 64+(6x32) = 256\n        out_channels = in_channels//2                                                          ### out_channels => 256//2 = 128\n        self.trans1 = Transition_Block(in_channels=in_channels, out_channels=out_channels)\n        \n        in_channels = out_channels                                                             ### in_channels = 128\n        \n        self.dense2 = self.make_layer(BottleNeck_Layer, in_channels=in_channels, growth_rate=growth_rate, num_layers=num_layers[1])\n        in_channels += num_layers[1]*growth_rate                                               ### in_channels => 128+(12x32) = 512\n        out_channels = in_channels//2                                                          ### out_channels => 512//2 = 256\n        self.trans2 = Transition_Block(in_channels=in_channels, out_channels=out_channels)\n        \n        in_channels = out_channels                                                             ### in_channels = 256\n        \n        self.dense3 = self.make_layer(BottleNeck_Layer, in_channels=in_channels, growth_rate=growth_rate, num_layers=num_layers[2])\n        in_channels += num_layers[2]*growth_rate                                               ### in_channels => 256+(24x32) = 1024\n        out_channels = in_channels//2                                                          ### out_channels => 1024//2 = 512\n        self.trans3 = Transition_Block(in_channels=in_channels, out_channels=out_channels)\n        \n        in_channels = out_channels                                                             ### in_channels = 512\n        \n        self.dense4 = self.make_layer(BottleNeck_Layer, in_channels=in_channels, growth_rate=growth_rate, num_layers=num_layers[3])\n        in_channels += num_layers[3]*growth_rate                                                ### in_channels => 512+(16x32) = 1024\n        #out_channels = in_channels//2                                                          ### out_channels => 1024//2 = 512\n        \n        self.bnorm = nn.BatchNorm2d(in_channels)\n        self.avgpool = nn.AvgPool2d(7)\n        self.flat = nn.Flatten()\n        self.fc = nn.Linear(in_channels, num_classes)\n        \n    def make_layer(self, block, in_channels, growth_rate, num_layers):            \n        \n        layers = []\n        for i in range(num_layers):\n            layers.append(block(in_channels, growth_rate))\n            in_channels += growth_rate\n            \n        return nn.Sequential(*layers)    \n    \n    def forward(self, x):\n        \n        out = self.conv1(x)\n        out = self.maxpool(out)\n        \n        out = self.dense1(out)\n        out = self.trans1(out)\n        \n        out = self.dense2(out)\n        out = self.trans2(out)\n        \n        out = self.dense3(out)\n        out = self.trans3(out)\n        \n        out = self.dense4(out)\n        out = self.bnorm(out)\n        \n        out = self.avgpool(out)\n        out = self.flat(out)\n        \n        out = self.fc(out)\n        \n        return out\n    \ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = DenseNet_Model_Torch(BottleNeck_Layer, [6, 12, 24, 16], 32, 1).to(device)                  ### DenseNet121 Architecture\nmodel = model.to(device)\n#loss_fn = nn.CrossEntropyLoss()\n#loss_fn = nn.BCELoss()\nloss_fn = nn.BCEWithLogitsLoss()\noptim = torch.optim.Adam(model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:44:50.847650Z","iopub.execute_input":"2023-09-17T20:44:50.847926Z","iopub.status.idle":"2023-09-17T20:44:53.907568Z","shell.execute_reply.started":"2023-09-17T20:44:50.847903Z","shell.execute_reply":"2023-09-17T20:44:53.906565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Note: \nSince we are using BCEWithLogitsLoss() we don't need to use a sigmoid function in the end, as opposed to using BCELoss()\n<br>Also since this is a binary classification problem we don't need to pass num_classes as 2 instead we can compute class probabilities of a single class and use it to predict labels","metadata":{}},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:44:53.909402Z","iopub.execute_input":"2023-09-17T20:44:53.910100Z","iopub.status.idle":"2023-09-17T20:44:53.921086Z","shell.execute_reply.started":"2023-09-17T20:44:53.910066Z","shell.execute_reply":"2023-09-17T20:44:53.919864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchsummary import summary\nsummary(model, (3, 224, 224))","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:44:53.922444Z","iopub.execute_input":"2023-09-17T20:44:53.923403Z","iopub.status.idle":"2023-09-17T20:44:58.845625Z","shell.execute_reply.started":"2023-09-17T20:44:53.923368Z","shell.execute_reply":"2023-09-17T20:44:58.844710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:44:58.848704Z","iopub.execute_input":"2023-09-17T20:44:58.849010Z","iopub.status.idle":"2023-09-17T20:44:58.859030Z","shell.execute_reply.started":"2023-09-17T20:44:58.848984Z","shell.execute_reply":"2023-09-17T20:44:58.858083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainSteps = len(train_dataloader.dataset)//batch_size\ntrainSteps","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:44:58.860098Z","iopub.execute_input":"2023-09-17T20:44:58.860609Z","iopub.status.idle":"2023-09-17T20:44:58.870877Z","shell.execute_reply.started":"2023-09-17T20:44:58.860576Z","shell.execute_reply":"2023-09-17T20:44:58.869704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valSteps = len(val_dataloader.dataset)//batch_size\nvalSteps","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:44:58.873697Z","iopub.execute_input":"2023-09-17T20:44:58.874050Z","iopub.status.idle":"2023-09-17T20:44:58.888054Z","shell.execute_reply.started":"2023-09-17T20:44:58.874020Z","shell.execute_reply":"2023-09-17T20:44:58.887205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ValidationLossEarlyStopping:\n    def __init__(self, patience=1, min_delta=0.0):\n        self.patience = patience  # number of times to allow for no improvement before stopping the execution\n        self.min_delta = min_delta  # the minimum change to be counted as improvement\n        self.counter = 0  # count the number of times the validation accuracy not improving\n        self.min_validation_loss = np.inf\n\n# return True when encountering _patience_ times decrease in validation loss \n    def early_stop_check(self, validation_loss):\n        if ((validation_loss+self.min_delta) < self.min_validation_loss):\n            self.min_validation_loss = validation_loss\n            self.counter = 0  # reset the counter if validation loss decreased at least by min_delta\n        elif ((validation_loss+self.min_delta) > self.min_validation_loss):\n            self.counter += 1 # increase the counter if validation loss is not decreased by the min_delta\n            if self.counter >= self.patience:\n                return True\n        return False\n\nearly_stopping = ValidationLossEarlyStopping(patience=5, min_delta=0.01)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:44:58.890412Z","iopub.execute_input":"2023-09-17T20:44:58.890665Z","iopub.status.idle":"2023-09-17T20:44:58.900109Z","shell.execute_reply.started":"2023-09-17T20:44:58.890643Z","shell.execute_reply":"2023-09-17T20:44:58.899183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ref.: https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch","metadata":{}},{"cell_type":"markdown","source":"# TRAINING MODEL","metadata":{}},{"cell_type":"code","source":"num_epochs = 50\ntrain_loss_list = []\nval_loss_list = []\ntrain_acc_list = []\nval_acc_list = []\ncount1 = len(train_dataloader.dataset)\ncount2 = len(val_dataloader.dataset)\n\nfor epoch in range(num_epochs):\n    epoch_acc = 0\n    train_loss = 0\n    val_loss = 0\n    val_epoch_acc = 0\n    for images, labels in train_dataloader:\n        #images = images.view(-1,1,28,28)\n        #count += len(labels)\n\n        images, labels = images.to(device), labels.to(device)\n\n        images = images.float()\n        y_pred = model(images)\n        #y_pred = y_pred.argmax(1)\n        y_pred = y_pred.squeeze(-1)\n        loss = loss_fn(y_pred, labels.float())\n\n        optim.zero_grad()\n        loss.backward()\n        optim.step()\n\n        train_loss += loss.item()\n        #print(y_pred)\n        y_pred = torch.tensor([1 if i>0 else 0 for i in y_pred]).to(device) \n        #print(y_pred)\n        epoch_acc += (y_pred == labels).sum().item()\n\n  ### Validation script\n\n    with torch.no_grad():\n        model.eval()\n\n        for val_images, val_labels in val_dataloader:\n        \n            val_images, val_labels = val_images.to(device), val_labels.to(device)\n\n            val_images = val_images.float()\n            y_pred_val = model(val_images)\n            y_pred_val = y_pred_val.squeeze(-1)\n            loss = loss_fn(y_pred_val, val_labels.float())\n\n            val_loss += loss.item()\n            y_pred_val = torch.tensor([1 if i>0 else 0 for i in y_pred_val]).to(device) \n            val_epoch_acc += (y_pred_val == val_labels).sum().item()\n        \n#     if early_stopping.early_stop_check(val_loss/valSteps):\n#         print(\"We are at epoch:\", epoch)\n#         break\n\n    print('Epoch: {} - Loss: {:.6f}, Training Acc.: {:.3%}, Val. Loss: {:.6f}, Validation Acc.: {:.3%}'.format(epoch + 1, train_loss/trainSteps, epoch_acc/count1, val_loss/valSteps, val_epoch_acc/count2))\n    train_loss_list.append(train_loss/trainSteps)\n    val_loss_list.append(val_loss/valSteps)\n    train_acc_list.append(epoch_acc/count1)\n    val_acc_list.append(val_epoch_acc/count2)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:44:58.901470Z","iopub.execute_input":"2023-09-17T20:44:58.902073Z","iopub.status.idle":"2023-09-17T20:47:05.457642Z","shell.execute_reply.started":"2023-09-17T20:44:58.902041Z","shell.execute_reply":"2023-09-17T20:47:05.456644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODEL PERFORMANCE","metadata":{}},{"cell_type":"code","source":"plt.plot(train_acc_list)\nplt.plot(val_acc_list)\n# plt.plot(history.history[\"val_loss\"])\nplt.title(\"Train & Val accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"epoch\")\nplt.legend([\"train\",\"test\"],loc=\"upper left\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:47:05.458969Z","iopub.execute_input":"2023-09-17T20:47:05.459620Z","iopub.status.idle":"2023-09-17T20:47:05.763503Z","shell.execute_reply.started":"2023-09-17T20:47:05.459585Z","shell.execute_reply":"2023-09-17T20:47:05.762645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(train_loss_list)\nplt.plot(val_loss_list)\n# plt.plot(history.history[\"val_loss\"])\nplt.title(\"Model Loss\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"epoch\")\nplt.legend([\"train\",\"test\"],loc=\"upper left\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:47:05.765031Z","iopub.execute_input":"2023-09-17T20:47:05.765675Z","iopub.status.idle":"2023-09-17T20:47:06.040596Z","shell.execute_reply.started":"2023-09-17T20:47:05.765639Z","shell.execute_reply":"2023-09-17T20:47:06.039688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SAMPLE RESULTS","metadata":{}},{"cell_type":"code","source":"model.eval()\n\nval_features, val_labels = next(iter(val_dataloader))\n\nval_images = val_images.to(device)\n        \nval_images = val_images.float()\ny_pred = model(val_images)\n\ny_pred_val = torch.tensor([1 if i>0 else 0 for i in y_pred_val]).to(device) \nprint(y_pred_val)\n\nfor i in range(len(val_images)):\n    plt.imshow(val_images[i].permute(1, 2, 0).cpu())\n    plt.title(f\"Actual: {encoded_labels[str(val_labels[i].item())]}, Pred: {encoded_labels[str(y_pred_val[i].item())]}\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:47:06.042066Z","iopub.execute_input":"2023-09-17T20:47:06.042471Z","iopub.status.idle":"2023-09-17T20:47:10.922579Z","shell.execute_reply.started":"2023-09-17T20:47:06.042436Z","shell.execute_reply":"2023-09-17T20:47:10.921702Z"},"trusted":true},"execution_count":null,"outputs":[]}]}